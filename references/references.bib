@article{mobilenetv2,
   abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the in-put/output domains from the expressiveness of the transformation , which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
   author = {Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
   title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
}
@article{mobilenetv1,
   abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification , face attributes and large scale geo-localization.},
   author = {Andrew G Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto},
   title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
}
@article{vit_cnn,
   abstract = {Transformers are models that implement a mechanism of self-attention, individually weighting the importance of each part of the input data. Their use in image classification tasks is still somewhat limited since researchers have so far chosen Convolutional Neural Networks for image classification and transformers were more targeted to Natural Language Processing (NLP) tasks. Therefore, this paper presents a literature review that shows the differences between Vision Transformers (ViT) and Convolutional Neural Networks. The state of the art that used the two architectures for image classification was reviewed and an attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes (for the classification problems), hardware, and evaluated architectures and top results. The objective of this work is to identify which of the architectures is the best for image classification and under what conditions. This paper also describes the importance of the Multi-Head Attention mechanism for improving the performance of ViT in image classification.},
   author = {José Maurício and Inês Domingues and Jorge Bernardino},
   doi = {10.3390/APP13095521},
   issn = {2076-3417},
   issue = {9},
   journal = {Applied Sciences 2023, Vol. 13, Page 5521},
   keywords = {Vision Transformers (ViT),convolutional neural networks,head attention,image classification,multi,transformers},
   month = {4},
   pages = {5521},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review},
   volume = {13},
   url = {https://www.mdpi.com/2076-3417/13/9/5521/htm https://www.mdpi.com/2076-3417/13/9/5521},
   year = {2023},
}
@article{imagenet,
   abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition , provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge , and propose future directions and improvements.},
   author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C Berg and Li Fei-Fei and O Russakovsky and J Deng and H Su and J Krause and S Satheesh and S Ma and Z Huang and A Karpathy and A Khosla and M Bernstein and A C Berg and L Fei-Fei},
   keywords = {Benchmark ·,Dataset ·,Large-scale ·,Object detection,Object recognition ·},
   title = {ImageNet Large Scale Visual Recognition Challenge},
   url = {http://image-net.org/challenges/LSVRC/},
}
@article{AL_imgs,
   abstract = {The integration of active learning (AL) and deep learning (DL) presents a promising avenue for enhancing the efficiency and performance of deep learning classifiers. This article introduces an approach that seamlessly integrates AL principles into the training process of DL models to build robust image classifiers. The proposed approach employs a unique methodology to select high-confidence unlabeled data points for immediate labeling, reducing the need for human annotation and minimizing annotation costs. Specifically, by combining uncertainty sampling with the pseudo-labeling of confident data, the proposed approach expands the training set efficiently. The proposed approach uses a hybrid active deep learning model that selects the most informative data points that need labeling based on an uncertainty measure. Then, it iteratively retrains a deep neural network classifier on the newly labeled samples. The model achieves high accuracy with fewer manually labeled samples than traditional supervised deep learning by selecting the most informative samples for labeling and retraining in a loop. Experiments on various image classification datasets demonstrate that the proposed model outperforms conventional approaches in terms of classification accuracy and reduced human annotation requirements. The proposed model achieved accuracy of 98.9% and 99.3% for the Cross-Age Celebrity and Caltech Image datasets compared to the conventional approach, which achieved 92.3% and 74.3%, respectively. In summary, this work presents a promising unified active deep learning approach to minimize the human effort in manually labeling data while maximizing classification accuracy by strategically labeling only the most valuable samples for the model.},
   author = {Amira Abdelwahab and Ahmed Afifi and Mohamed Salama},
   doi = {10.3390/ELECTRONICS13010169},
   issn = {2079-9292},
   issue = {1},
   journal = {Electronics 2024, Vol. 13, Page 169},
   keywords = {active learning,annotation costs,deep learning,image classification,query strategies,unlabeled data classification},
   month = {12},
   pages = {169},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {An Integrated Active Deep Learning Approach for Image Classification from Unlabeled Data with Minimal Supervision},
   volume = {13},
   url = {https://www.mdpi.com/2079-9292/13/1/169/htm https://www.mdpi.com/2079-9292/13/1/169},
   year = {2023},
}
@book{bailey,
   author = {Donald G. Bailey},
   title = {Design for Embedded Image Processing on FPGAs},
   year = {2023},
}
@book{harris_harris,
   abstract = {have taken the popular pedagogy from Computer Organization and Design to the next level of refinement, showing in detail how to build a MIPS microprocessor in both SystemVerilog and VHDL. With the exciting opportunity that students have to run large digital designs on modern FGPAs, the approach the authors take in this book is both informative and enlightening.},
   author = {David M. Harris and Sarah Harris},
   title = {Digital Design and Computer Architecture 2nd edition},
   year = {2012},
}
@article{wm-811k,
   abstract = {In this study, we worked on how to automate the wafer map failure pattern classification using deep learning computer vision methods, which is a hot topic in the semiconductor industry nowadays. We preprocessed the pre-existing public dataset using data normalization and augmentation, explored both simplified AlexNet and simplified VGG16 model architectures, performed hyperparameter tuning to optimize our model performance, and reported the quantitative and qualitative results of our two models. It turns out that our simplified versions of AlexNet and VGG16 models can both achieve high accuracy, precision and recall, with simplified VGG16 outperforming simplified AlexNet.},
   author = {Jie Gong},
   title = {Wafer Map Failure Pattern Classification Using Deep Learning},
   year = {2019},
}
@article{brain_tumor,
   abstract = {A Brain tumor is one aggressive disease. An estimated more than 84,000 people will receive a primary brain tumor diagnosis in 2021 and an estimated 18,600 people will die from a malignant brain tumor (brain cancer) in 2021.[8] The best technique to detect brain tumors is by using Magnetic Resonance Imaging (MRI). More than any other cancer, brain tumors can have lasting and life-altering physical, cognitive, and psychological impacts on a patient's life and hence faster diagnosis and best treatment plan should be devised to improve the life expectancy and well-being of these patients. Neural networks have shown colossal accuracy in image classification and segmentation problems. In this paper, we propose comparative studies of various deep learning models based on different types of Neural Networks(ANN, CNN, TL) to firstly identify brain tumors and then classify them into Benign Tumor, Malignant Tumor or Pituitary Tumor. The data set used holds 3190 images on T1-weighted contrast-enhanced images which were cleaned and augmented. The best ANN model concluded with an accuracy of 78% and the best CNN model consisting of 3 convolution layers had an accuracy of 90%. The VGG16(re-trained on the dataset) model surpasses other ANN, CNN, TL models for multi-class tumor classification. This proposed network achieves significantly better performance with a validation accuracy of 94% and an F1-Score of 91.},
   author = {Ankita Kadam and Sartaj Bhuvaji and Sujit Deshpande},
   journal = {International Journal for Research in Applied Science & Engineering Technology (IJRASET)},
   keywords = {Artificial Neural Network(ANN),Convolution Neural Network (CNN),Magnetic Resonance Imaging(MRI),Transfer Learning(TL)},
   pages = {2321-9653},
   title = {Brain Tumor Classification using Deep Learning Algorithms},
   volume = {9},
   url = {www.ijraset.com417},
   year = {2021},
}
@article{hw_ieee,
   author = {Pudi Dhilleswararao and Graduate Student Member and Srinivas Boppu and M Sabarimalai Manikandan and Senior Member and Linga Reddy Cenkeramaddi},
   doi = {10.1109/ACCESS.2022.3229767},
   title = {Efficient Hardware Architectures for Accelerating Deep Neural Networks: Survey},
}
@article{hw_img1,
   author = {Dimitrios Danopoulos and Christoforos Kachris and Dimitrios Soudris},
   doi = {10.1109/MOCAST.2018.8376580},
   isbn = {978-1-5386-4788-2},
   journal = {2018 7th International Conference on Modern Circuits and Systems Technologies (MOCAST)},
   month = {5},
   pages = {1-4},
   publisher = {IEEE},
   title = {Acceleration of Image Classification with Caffe Framework Using FPGA},
   year = {2018},
}
@article{hw_perez_figueroa_2021,
   abstract = {Convolutional neural networks (CNN) have been extensively employed for image classification due to their high accuracy. However, inference is a computationally-intensive process that often requires hardware acceleration to operate in real time. For mobile devices, the power consumption of graphics processors (GPUs) is frequently prohibitive, and field-programmable gate arrays (FPGA) become a solution to perform inference at high speed. Although previous works have implemented CNN inference on FPGAs, their high utilization of on-chip memory and arithmetic resources complicate their application on resource-constrained edge devices. In this paper, we present a scalable, low power, low resource-utilization accelerator architecture for inference on the MobileNet V2 CNN. The architecture uses a heterogeneous system with an embedded processor as the main controller, external memory to store network data, and dedicated hardware implemented on reconfigurable logic with a scalable number of processing elements (PE). Implemented on a XCZU7EV FPGA running at 200 MHz and using four PEs, the accelerator infers with 87% top-5 accuracy and processes an image of 224×224 pixels in 220 ms. It consumes 7.35 W of power and uses less than 30% of the logic and arithmetic resources used by other MobileNet FPGA accelerators.},
   author = {Ignacio Pérez and Miguel Figueroa and Stefania Perri},
   doi = {10.3390/S21082637},
   issn = {1424-8220},
   issue = {8},
   journal = {Sensors 2021, Vol. 21, Page 2637},
   keywords = {MobileNet V2,convolutional neural network,field,power consumption,programmable gate array},
   month = {4},
   pages = {2637},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {A Heterogeneous Hardware Accelerator for Image Classification in Embedded Systems},
   volume = {21},
   url = {https://www.mdpi.com/1424-8220/21/8/2637/htm https://www.mdpi.com/1424-8220/21/8/2637},
   year = {2021},
}
@article{hw3,
   abstract = {Machine learning (ML) and artificial intelligence (AI) technology are revolutionizing many different fields of study in computer science as well as a wide range of industry sectors such as information technology, mobile communication, automotive, and manufacturing. As more people are using the technology in their everyday life, the demand for new hardware that enables faster and more energy-efficient AI processing is ever increasing. Over the last few years, traditional hardware makers such as Intel and Nvidia as well as start-up companies such as Graphcore and Habana Labs were trying to offer the best computing platform for complex AI workloads. Although GPU still remains the most preferred platform due to its generic programming interface, it is certainly not suitable for mobile/edge applications due to its low hardware utilization and huge power consumption. On the other hand, FPGA is a promising hardware platform for accelerating deep neural networks (DNNs) thanks to its re-programmability and power efficiency. In this chapter, we review essential computations in latest DNN models and their algorithmic optimizations. We then investigate various accelerator architectures based on FPGAs and design automation frameworks. Finally, we discuss the device's strengths and weaknesses over other types of hardware platforms and conclude with future research directions.},
   author = {Joo Young Kim},
   doi = {10.1016/BS.ADCOM.2020.11.002},
   isbn = {9780128231234},
   issn = {0065-2458},
   journal = {Advances in Computers},
   keywords = {Accelerator,Artificial intelligence,Computer architecture,Deep neural networks,Design automation,FPGA,Machine learning},
   month = {1},
   pages = {135-165},
   publisher = {Elsevier},
   title = {FPGA based neural network accelerators},
   volume = {122},
   year = {2021},
}
@article{hw2,
   abstract = {Recent researches on neural network have shown significant advantage in machine learning over traditional algorithms based on handcrafted features and models. Neural network is now widely adopted in regions like image, speech and video recognition. But the high computation and storage complexity of neural network inference poses great difficulty on its application. CPU platforms are hard to offer enough computation capacity. GPU platforms are the first choice for neural network process because of its high computation capacity and easy to use development frameworks. On the other hand, FPGA-based neural network inference accelerator is becoming a research topic. With specifically designed hardware, FPGA is the next possible solution to surpass GPU in speed and energy efficiency. Various FPGA-based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency. In this paper, we give an overview of previous work on neural network inference accelerators based on FPGA and summarize the main techniques used. An investigation from software to hardware, from circuit level to system level is carried out to complete analysis of FPGA-based neural network inference accelerator design and serves as a guide to future work.},
   author = {Kaiyuan Guo and Shulin Zeng and Jincheng Yu and Y U Wang and Huazhong Yang and Yu Wang},
   issue = {11},
   journal = {ACM Trans. Reconng. Technol. Syst},
   keywords = {Additional Key Words and Phrases: FPGA architecture, Neural Network, Parallel Processing ACM Reference format:,CCS Concepts: •General and reference → Surveys and overviews,•Computer systems organization → Parallel architectures},
   month = {12},
   title = {A Survey of FPGA-Based Neural Network Accelerator},
   volume = {9},
   url = {https://arxiv.org/abs/1712.08934v3},
   year = {2017},
}
@article{hw1,
   abstract = {Deep learning, the fastest growing segment of Artificial Neural Network (ANN), has led to the emergence of many machine learning applications and their implementation across multiple platforms such as CPUs, GPUs and reconfigurable hardware (Field-Programmable Gate Arrays or FPGAs). However, inspired by the structure and function of ANNs, large-scale deep learning topologies require a considerable amount of parallel processing, memory resources, high throughput and significant processing power. Consequently, in the context of real time hardware systems, it is crucial to find the right trade-off between performance, energy efficiency, fast development, and cost. Although limited in size and resources, several approaches have showed that FPGAs provide a good starting point for the development of future deep learning implementation architectures. Through this paper, we briefly review recent work related to the implementation of deep learning algorithms in FPGAs. We will analyze and compare the design requirements and features of existing topologies to finally propose development strategies and implementation architectures for better use of FPGA-based deep learning topologies. In this context, we will examine the frameworks used in these studies, which will allow testing a lot of topologies to finally arrive at the best implementation alternatives in terms of performance and energy efficiency.},
   author = {Ahmed Ghazi Blaiech and Khaled Ben Khalifa and Carlos Valderrama and Marcelo A.C. Fernandes and Mohamed Hedi Bedoui},
   doi = {10.1016/J.SYSARC.2019.01.007},
   issn = {1383-7621},
   journal = {Journal of Systems Architecture},
   keywords = {Deep learning,FPGA,Framework,Optimized implementation},
   month = {9},
   pages = {331-345},
   publisher = {North-Holland},
   title = {A Survey and Taxonomy of FPGA-based Deep Learning Accelerators},
   volume = {98},
   year = {2019},
}
@article{naive_bayes,
   abstract = {Naïve Bayes (NB) is a well-known probabilistic classification algorithm. It is a simple but efficient algorithm with a wide variety of real-world applications, ranging from product recommendations through medical diagnosis to controlling autonomous vehicles. Due to the failure of real data satisfying the assumptions of NB, there are available variations of NB to cater general data. With the unique applications for each variation of NB, they reach different levels of accuracy. This manuscript surveys the latest applications of NB and discusses its variations in different settings. Furthermore, recommendations are made regarding the applicability of NB while exploring the robustness of the algorithm. Finally, an attempt is given to discuss the pros and cons of NB algorithm and some vulnerabilities, with related computing code for implementation.},
   author = {Indika Wickramasinghe and Harsha Kalutarage},
   doi = {10.1007/S00500-020-05297-6/FIGURES/2},
   issn = {14337479},
   issue = {3},
   journal = {Soft Computing},
   keywords = {Machine learning vulnerabilities,Naïve Bayes,Probabilistic classification,R code snippets},
   month = {2},
   pages = {2277-2293},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Naive Bayes: applications, variations and vulnerabilities: a review of literature with code snippets for implementation},
   volume = {25},
   url = {https://link.springer.com/article/10.1007/s00500-020-05297-6},
   year = {2021},
}
@misc{gelu,
   abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is xΦ(x), where Φ(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1 x>0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
   author = {Dan Hendrycks and Kevin Gimpel},
   title = {Gaussian Error Linear Units (GELUs)},
   year = {2016},
}
@article{qbc,
   abstract = {We propose an algorithm called query by committee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.},
   author = {H. S. Seung and M. Opper and H. Sompolinsky},
   doi = {10.1145/130385.130417},
   isbn = {089791497X},
   journal = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
   pages = {287-294},
   publisher = {Publ by ACM},
   title = {Query by committee},
   url = {https://collaborate.princeton.edu/en/publications/query-by-committee},
   year = {1992},
}
@article{alsurvey2023,
   abstract = {Despite the availability and ease of collecting a large amount of free, unlabeled data, the expensive and time-consuming labeling process is still an obstacle to labeling a sufficient amount of training data, which is essential for building supervised learning models. Here, with low labeling cost, the active learning (AL) technique could be a solution, whereby a few, high-quality data points are queried by searching for the most informative and representative points within the instance space. This strategy ensures high generalizability across the space and improves classification performance on data we have never seen before. In this paper, we provide a survey of recent studies on active learning in the context of classification. This survey starts with an introduction to the theoretical background of the AL technique, AL scenarios, AL components supported with visual explanations, and illustrative examples to explain how AL simply works and the benefits of using AL. In addition to an overview of the query strategies for the classification scenarios, this survey provides a high-level summary to explain various practical challenges with AL in real-world settings; it also explains how AL can be combined with various research areas. Finally, the most commonly used AL software packages and experimental evaluation metrics with AL are also discussed.},
   author = {Alaa Tharwat and Wolfram Schenck},
   doi = {10.3390/MATH11040820},
   issn = {2227-7390},
   issue = {4},
   journal = {Mathematics 2023, Vol. 11, Page 820},
   keywords = {active learning,query strategy,semi,supervised learning},
   month = {2},
   pages = {820},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {A Survey on Active Learning: State-of-the-Art, Practical Challenges and Research Directions},
   volume = {11},
   url = {https://www.mdpi.com/2227-7390/11/4/820/htm https://www.mdpi.com/2227-7390/11/4/820},
   year = {2023},
}
@article{margins,
   abstract = {Information extraction from HTML documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted. If completely labeled documents are available for training, well-known Markov model techniques can be used to learn...},
   author = {Tobias Scheffer and Christian Decomain and Stefan Wrobel},
   doi = {10.1007/3-540-44816-0_31},
   isbn = {978-3-540-44816-7},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {309-318},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Active Hidden Markov Models for Information Extraction},
   volume = {2189},
   url = {https://link.springer.com/chapter/10.1007/3-540-44816-0_31},
   year = {2001},
}
@article{settles2009,
   author = {Burr Settles},
   keywords = {active learning,literature survey,machine learning},
   title = {Computer Sciences Department Active Learning Literature Survey},
   year = {2009},
}
@article{settles2008,
   abstract = {Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmen-tation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.},
   author = {Burr Settles and Mark Craven},
   publisher = {ACL Press},
   title = {An Analysis of Active Learning Strategies for Sequence Labeling Tasks},
   year = {2008},
}
@article{lewis_gale,
   abstract = {The ability to cheaply train text classiiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classiiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classiied to achieve a given level of eeectiveness.},
   author = {David D Lewis and William A Gale and In W Bruce Croft and C J Van Rijsbergen},
   pages = {3-12},
   publisher = {Springer-Verlag},
   title = {A Sequential Algorithm for Training Text Classiiers},
   year = {1994},
}
@article{clust_2017,
   abstract = {Active learning is used for classification when labeling data are costly, while the main challenge is to identify the critical instances that should be labeled. Clustering-based approaches take advantage of the structure of the data to select representative instances. In this paper, we developed the active learning through density peak clustering (ALEC) algorithm with three new features. First, a master tree was built to express the relationships among the nodes and assist the growth of the cluster tree. Second, a deterministic instance selection strategy was designed using a new importance measure. Third, tri-partitioning was employed to determine the action to be taken on each instance during iterative clustering, labeling, and classifying. Experiments were performed with 14 datasets to compare against state-of-the-art active learning algorithms. Results demonstrated that the new algorithm had higher classification accuracy using the same number of labeled data.},
   author = {Min Wang and Fan Min and Zhi Heng Zhang and Yan Xue Wu},
   doi = {10.1016/J.ESWA.2017.05.046},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Active learning,Classification,Density clustering,Master tree,Tri-partitioning},
   month = {11},
   pages = {305-317},
   publisher = {Pergamon},
   title = {Active learning through density clustering},
   volume = {85},
   year = {2017},
}
@article{clust_mnist,
   abstract = {Active learning is a type of semi-supervised learning in which the training algorithm is able to obtain the labels of a small portion of the unlabeled dataset by interacting with an external source (e.g. a human annotator). One strategy employed in active learning is...},
   author = {Saul Berardo and Eloi Favero and Nelson Neto},
   doi = {10.1007/978-3-319-18356-5_25},
   isbn = {978-3-319-18356-5},
   issn = {1611-3349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Active learning,Clustering,Unsupervised feature learning},
   pages = {281-290},
   publisher = {Springer, Cham},
   title = {Active Learning with Clustering and Unsupervised Feature Learning},
   volume = {9091},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-18356-5_25},
   year = {2015},
}
@article{clust_1,
   abstract = {We propose a method of selecting initial training examples for active learning so that it can reach high performance faster with fewer further queries. Our method divides the unlabeled examples into clusters of similar ones and then selects from each cluster the most...},
   author = {Jaeho Kang and Kwang Ryel Ryu and Hyuk Chul Kwon},
   doi = {10.1007/978-3-540-24775-3_46},
   isbn = {978-3-540-24775-3},
   issn = {1611-3349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {384-388},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Using Cluster-Based Sampling to Select Initial Training Set for Active Learning in Text Classification},
   volume = {3056},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-24775-3_46},
   year = {2004},
}
@article{lenet,
   author = {Yann Lecun and L Eon Bottou and Yoshua Bengio and Patrick Haaner Abstract|},
   title = {Gradient-Based Learning Applied to Document Recognition},
   year = {1998},
}
@article{attention_is_all_you_need,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {5999-6009},
   publisher = {Neural information processing systems foundation},
   title = {Attention Is All You Need},
   volume = {2017-December},
   url = {https://arxiv.org/abs/1706.03762v7},
   year = {2017},
}
@misc{rmsprop,
   author = {Geoffrey Hinton and Nitish Srivastava and Kevin Swersky},
   title = {Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent},
   year = {2012},
}
@article{adam,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P Kingma and Jimmy Lei Ba},
   title = {Adam: A Method for Stochastic Optimization},
}
@article{adagrad,
   abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
   author = {John Duchi JDUCHI and Yoram Singer},
   journal = {Journal of Machine Learning Research},
   keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
   pages = {2121-2159},
   title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan},
   volume = {12},
   year = {2011},
}
@article{regularization_survey,
   abstract = {In machine learning, the model is not as complicated as possible. Good generalization ability means that the model not only performs well on the training data set, but also can make good prediction on new data. Regularization imposes a penalty on model's complexity or smoothness, allowing for good generalization to unseen data even when training on a finite training set or with an inadequate iteration. Deep learning has developed rapidly in recent years. Then the regularization has a broader definition: regularization is a technology aimed at improving the generalization ability of a model. This paper gave a comprehensive study and a state-of-the-art review of the regularization strategies in machine learning. Then the characteristics and comparisons of regularizations were presented. In addition, it discussed how to choose a regularization for the specific task. For specific tasks, it is necessary for regularization technology to have good mathematical characteristics. Meanwhile, new regularization techniques can be constructed by extending and combining existing regularization techniques. Finally, it concluded current opportunities and challenges of regularization technologies, as well as many open concerns and research trends.},
   author = {Yingjie Tian and Yuqi Zhang},
   doi = {10.1016/j.inffus.2021.11.005},
   journal = {Information Fusion},
   keywords = {Generalization,Machine learning,Overfitting,Regularization},
   pages = {146-166},
   title = {A comprehensive survey on regularization strategies in machine learning},
   volume = {80},
   url = {https://doi.org/10.1016/j.inffus.2021.11.005},
   year = {2022},
}
@article{cnn_review,
   abstract = {Image classification has always been a hot research direction in the world, and the emergence of deep learning has promoted the development of this field. Convolutional neural networks (CNNs) have gradually become the mainstream algorithm for image classification since 2012, and the CNN architecture applied to other visual recognition tasks (such as object detection, object localization, and semantic segmentation) is generally derived from the network architecture in image classification. In the wake of these successes, CNN-based methods have emerged in remote sensing image scene classification and achieved advanced classification accuracy. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art (SOAT) network architectures. Along the way, we analyze (1) the basic structure of artificial neural networks (ANNs) and the basic network layers of CNNs, (2) the classic predecessor network models, (3) the recent SOAT network algorithms, (4) comprehensive comparison of various image classification methods mentioned in this article. Finally, we have also summarized the main analysis and discussion in this article, as well as introduce some of the current trends.},
   author = {Leiyu Chen and Shaobo Li and Qiang Bai and Jing Yang and Sanlong Jiang and Yanming Miao},
   doi = {10.3390/RS13224712},
   issn = {2072-4292},
   issue = {22},
   journal = {Remote Sensing 2021, Vol. 13, Page 4712},
   keywords = {convolutional neural networks,deep learning,image classification},
   month = {11},
   pages = {4712},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Review of Image Classification Algorithms Based on Convolutional Neural Networks},
   volume = {13},
   url = {https://www.mdpi.com/2072-4292/13/22/4712/htm https://www.mdpi.com/2072-4292/13/22/4712},
   year = {2021},
}
@article{lrelu,
   abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
   author = {Andrew L Maas and Awni Y Hannun and Andrew Y Ng},
   title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
   year = {2013},
}
@article{afs,
   abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.},
   author = {Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
   doi = {10.1016/J.NEUCOM.2022.06.111},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Activation Functions,Convolutional neural networks,Deep learning,Neural networks,Overview,Recurrent Neural Networks},
   month = {9},
   pages = {92-108},
   publisher = {Elsevier},
   title = {Activation functions in deep learning: A comprehensive survey and benchmark},
   volume = {503},
   year = {2022},
}
@article{relu,
   abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hy-perbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
   author = {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
   title = {Deep Sparse Rectifier Neural Networks},
   year = {2011},
}
@article{vit,
   abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
   author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
   journal = {ICLR 2021 - 9th International Conference on Learning Representation},
   month = {10},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
   url = {https://arxiv.org/abs/2010.11929v2},
   year = {2021},
}
@article{dt_2,
   author = {by J Ross Quinlan and Morgan Kaufmann Publishers and Steven L Salzberg},
   pages = {235-240},
   title = {Programs for Machine Learning},
   volume = {16},
   year = {1994},
}
@article{dt_1,
   abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
   author = {J R Quinlan},
   journal = {Machine Learning},
   keywords = {classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
   pages = {81-106},
   title = {Induction of Decision Trees},
   volume = {1},
   year = {1986},
}
@article{random_forests,
   author = {Leo Breiman},
   doi = {10.1023/A:1010933404324},
   issn = {08856125},
   issue = {1},
   journal = {Machine Learning},
   pages = {5-32},
   title = {Random Forests},
   volume = {45},
   year = {2001},
}
@article{knn,
   author = {T. Cover and P. Hart},
   doi = {10.1109/TIT.1967.1053964},
   issn = {0018-9448},
   issue = {1},
   journal = {IEEE Transactions on Information Theory},
   month = {1},
   pages = {21-27},
   title = {Nearest neighbor pattern classification},
   volume = {13},
   year = {1967},
}
@article{bovw,
   abstract = {We present a novel method for generic visual catego-rization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naïve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.},
   author = {Gabriella Csurka and Christopher R Dance and Lixin Fan and Jutta Willamowski and Cédric Bray},
   title = {Visual Categorization with Bags of Keypoints},
}
@article{haar,
   abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
   author = {Paul Viola and Michael Jones},
   doi = {10.1109/CVPR.2001.990517},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Rapid object detection using a boosted cascade of simple features},
   volume = {1},
   year = {2001},
}
@inproceedings{hog,
   author = {N. Dalal and B. Triggs},
   doi = {10.1109/CVPR.2005.177},
   isbn = {0-7695-2372-2},
   booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
   pages = {886-893},
   publisher = {IEEE},
   title = {Histograms of Oriented Gradients for Human Detection},
   year = {2005},
}
@inproceedings{lbp,
   author = {T. Ojala and M. Pietikainen and D. Harwood},
   doi = {10.1109/ICPR.1994.576366},
   isbn = {0-8186-6265-4},
   booktitle = {Proceedings of 12th International Conference on Pattern Recognition},
   pages = {582-585},
   publisher = {IEEE Comput. Soc. Press},
   title = {Performance evaluation of texture measures with classification based on Kullback discrimination of distributions},
   year = {1994},
}
@book{gonzales_wood,
   abstract = {Fourth edition. "For 40 years, Image Processing has been the foundational text for the study of digital image processing. The book is suited for students at the college senior and first-year graduate level with prior background in mathematical analysis, vectors, matrices, probability, statistics, linear systems, and computer programming. As in all earlier editions, the focus of this edition of the book is on fundamentals. The 4th Edition, which celebrates the book's 40th anniversary, is based on an extensive survey of faculty, students, and independent readers in 150 institutions from 30 countries. Their feedback led to expanded or new coverage of topics such as deep learning and deep neural networks, including convolutional neural nets, the scale-invariant feature transform (SIFT), maximally-stable extremal regions (MSERs), graph cuts, k-means clustering and superpixels, active contours (snakes and level sets), and exact histogram matching. Major improvements were made in reorganizing the material on image transforms into a more cohesive presentation, and in the discussion of spatial kernels and spatial filtering. Major revisions and additions were made to examples and homework exercises throughout the book. For the first time, we added MATLAB projects at the end of every chapter, and compiled support packages for you and your teacher containing, solutions, image databases, and sample code."--Amazon.com. Introduction -- Digital image fundamentals -- Intensity transformations and spatial filtering -- Filtering in the frequency domain -- Image restoration and reconstruction -- Wavelet and other image transforms -- Color image processing -- Image compression and watermarking -- Morphological image processing -- Image segmentation I -- Image segmentation II active contours : snakes and level sets -- Feature extraction -- Image pattern classification.},
   author = {Rafael C. Gonzalez and Richard E. (Richard Eugene) Woods},
   edition = {4th},
   isbn = {9780133356724},
   pages = {1168},
   title = {Digital image processing},
}
@misc{svm,
   abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
   author = {Corinna Cortes},
   keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
   pages = {273-297},
   title = {Support-Vector Networks},
   volume = {20},
   year = {1995},
}
@article{vggnet,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   month = {9},
   title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
   url = {http://arxiv.org/abs/1409.1556},
   year = {2014},
}
@article{alexnet,
   abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
   author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
   doi = {10.1145/3065386},
   issn = {15577317},
   issue = {6},
   journal = {Communications of the ACM},
   month = {6},
   pages = {84-90},
   publisher = {Association for Computing Machinery},
   title = {ImageNet classification with deep convolutional neural networks},
   volume = {60},
   year = {2017},
}
@article{surf,
   abstract = {In this paper, we present a novel scale-and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descrip-tors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
   author = {Herbert Bay and Tinne Tuytelaars and Luc Van Gool},
   title = {SURF: Speeded Up Robust Features},
   year = {2006},
}
@article{resnet,
   abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
   author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
   month = {12},
   title = {Deep Residual Learning for Image Recognition},
   url = {http://arxiv.org/abs/1512.03385},
   year = {2015},
}
@article{sift,
   abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
   author = {David G Lowe},
   title = {Object Recognition from Local Scale-Invariant Features},
   year = {1999},
}
@article{inception,
   abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
   author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
   title = {Going Deeper with Convolutions},
   year = {2014},
}
@article{,
   abstract = {The past decade has seen the growing popularity of Bag of Features (BoF) approaches to many computer vision tasks, including image classification, video search, robot localization, and texture recognition. Part of the appeal is simplicity. BoF methods are based on orderless collections of quantized local image descriptors; they discard spatial information and are therefore conceptually and computationally simpler than many alternative methods. Despite this, or perhaps because of this, BoF-based systems have set new performance standards on popular image classification benchmarks and have achieved scalability breakthroughs in image retrieval. This paper presents an introduction to BoF image representations, describes critical design choices, and surveys the BoF literature. Emphasis is placed on recent techniques that mitigate quantization errors, improve feature detection, and speed up image retrieval. At the same time, unresolved issues and fundamental challenges are raised. Among the unresolved issues are determining the best techniques for sampling images, describing local image features, and evaluating system performance. Among the more fundamental challenges are how and whether BoF methods can contribute to localizing objects in complex images, or to associating high-level semantics with natural images. This survey should be useful both for introducing new investigators to the field and for providing existing researchers with a consolidated reference to related work.},
   author = {Stephen O'Hara and Bruce A. Draper},
   month = {1},
   title = {Introduction to the Bag of Features Paradigm for Image Classification and Retrieval},
   url = {http://arxiv.org/abs/1101.3354},
   year = {2011},
}
@article{feature_extraction,
   abstract = {Feature plays a very important role in the area of image processing. Before getting features, various image preprocessing techniques like binarization, thresholding, resizing, normalization etc. are applied on the sampled image. After that, feature extraction techniques are applied to get features that will be useful in classifying and recognition of images. Feature extraction techniques are helpful in various image processing applications e.g. character recognition. As features define the behavior of an image, they show its place in terms of storage taken, efficiency in classification and obviously in time consumption also. Here in this paper, we are going to discuss various types of features, feature extraction techniques and explaining in what scenario, which features extraction technique, will be better. Hereby in this paper, we are going to refer features and feature extraction methods in case of character recognition application. © 2014 IEEE.},
   author = {Gaurav Kumar and Pradeep Kumar Bhatia},
   doi = {10.1109/ACCT.2014.74},
   isbn = {9781479949106},
   issn = {23270659},
   journal = {International Conference on Advanced Computing and Communication Technologies, ACCT},
   keywords = {Feature Extraction,Image Processing,Optical Character Recognition (OCR),Pattern Recognition},
   pages = {5-12},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Detailed Review of Feature Extraction in Image Processing Systems},
   year = {2014},
}
@book{moelgendy,
   abstract = {1st edition. How does the computer learn to understand what it sees? Deep Learning for Vision Systems answers that by applying deep learning to computer vision. Using only high school algebra, this book illuminates the concepts behind visual intuition. You'll understand how to use deep learning architectures to build vision system applications for image generation and facial recognition.},
   author = {Mohamed Elgendy and O'Reilly for Higher Education (Firm) and an O'Reilly Media Company. Safari},
   isbn = {9781617296192},
   pages = {480},
   title = {Deep Learning for Vision Systems},
}
